inputs:
- variable: dataset_size__llama3__tokens
  variable_type: dataset_size
  entity: llama3
  units: tokens
  nice_name: Total pre-training tokens (Llama 3)
  value: 15000000000000.0
  scale: 1e9
  display_units: billions of tokens
  value_description: Total tokens used to pre-training a model
  key_assumption: > 
    We can use 15 trillion as a default value based on the assumption that most frontier models use roughly same pre-training size.
    15T is the number cited in Llama3 model card and close to the FineWeb size.
  source_url: https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md
  source_notes: Source is the Llama 3 model card. It describes total number of tokens for pre-training.
  
- variable: yearly_revenue__openai__dollars
  variable_type: yearly_revenue
  entity: openai
  units: dollars
  nice_name: Revenue from AI (OpenAI)
  value: 3490000000.0
  scale: 1e6
  display_units: millions of dollars
  value_description: Value generated by AI in dollars
  key_assumption: Assuming all 2024 OpenAI revenue is from LLMs
  source_url: https://www.bloomberg.com/news/articles/2024-06-12/openai-doubles-annualized-revenue-to-3-4-billion-information
  source_notes: business news coverage of openai
  related_inputs:
    - yearly_revenue__anthropic__dollars

- variable: yearly_revenue__anthropic__dollars
  variable_type: yearly_revenue
  entity: anthropic
  units: dollars
  nice_name: Revenue from AI (Anthropic)
  value: 1000000000.0
  units: dollars
  scale: 1e6
  display_units: millions of dollars
  value_description: Value generated by AI in dollars
  key_assumption: assuming all 2024 Anthropic revenue is from LLMs
  source_url: https://www.pymnts.com/artificial-intelligence-2/2024/anthropic-revenue-reportedly-set-to-jump-to-1-billion-this-year/
  source_notes: business news coverage of anthropic
  related_inputs:
    - yearly_revenue__openai__dollars

- variable: average_tokens_per_contribution__redpajama__tokens_per_contribution
  nice_name: Average number of a token in a single document
  value: 1413.0
  units: tokens / contribution
  value_description: Average number of tokens in a single 'contribution'
  variable_type: dataset
  confidence_in_number: high
  key_assumption: working with averages here
  source_url: https://github.com/togethercomputer/RedPajama-Data
  source_notes: RedPajama readme reports a ratio of documents to tokens (after dedupe). We use the English figures (20.5T tokens / 14.5B documents)

- variable: total_books__books3__books
  value: 196640.0
  units: books
  value_description: Number of books in Books3
  variable_type: dataset
  confidence_in_number: high
  key_assumption: N/A
  source_url: https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/books3-ai-training-dataset
  source_notes: Entry in the AIAAIC repository.

- variable: average_length__book__words
  value: 80000.0
  units: words
  value_description: An average number of words per book
  variable_type: dataset
  confidence_in_number: moderate
  key_assumption: Average number of words per book
  source_url: https://www.penguin.co.uk/articles/2020/09/book-length-debate-fiction-long-novels
  source_notes: No additional notes.

- variable: words_per_token__generic__words_per_token
  value: 0.75
  units: words / token
  value_description: average number of words per token
  variable_type: training_detail
  confidence_in_number: moderate
  key_assumption: Average across random queries
  source_url: https://platform.openai.com/tokenizer
  source_notes: OpenAI Tokenizer

- variable: freelance_rate_low__generic__dollars_per_word
  value: 0.05
  units: dollars / word
  value_description: Freelance rate per word (low estimate)
  variable_type: wage_data
  confidence_in_number: unknown
  key_assumption: '...'
  source_url: source needed
  source_notes: source needed

- variable: freelance_rate_high__generic__dollars_per_word
  value: 0.1
  units: dollars / word
  value_description: Freelance rate per word (high estimate)
  variable_type: wage_date
  confidence_in_number: unknown
  key_assumption: assumptions needed
  source_url: source needed
  source_notes: source needed

- variable: daily_active_users__reddit__users
  value: 267500000.0
  units: daily active users
  scale: 1e6
  display_units: thousands of users
  value_description: Number of Reddit daily active users
  variable_type: group_size
  confidence_in_number: high
  key_assumption: None
  source_url: https://backlinko.com/reddit-users
  source_notes: 'article about # of reddit users'
  
- variable: deal_value__reddit_google__millions_dollar
  value: 60000000.0
  units: dollars
  scale: 1e6
  display_unit: millions of dollars
  value_description: Payment made to Reddit by Google
  variable_type: deal_data
  confidence_in_number: high
  key_assumption: None
  source_url: https://www.cbsnews.com/news/google-reddit-60-million-deal-ai-training/
  source_notes: CBS news coverage

- variable: deal_value__taylorandfrancis_microsoft__millions_dollars
  value: 10.0
  units: millions of dollars
  value_description: Value of the Taylor and Francis deal
  variable_type: deal_data
  confidence_in_number: high
  key_assumption: None
  source_url: https://www.thebookseller.com/news/academic-authors-shocked-after-taylor--francis-sells-access-to-their-research-to-microsoft-ai
  source_notes: news coverage of deal

- variable: deal_value__newscorp__dollars
  value: 50000000.0
  units: dollars
  value_description: Value of the News Corp deal
  variable_type: deal_date
  confidence_in_number: high
  key_assumption: Not specified
  source_url: https://www.nytimes.com/2024/05/22/business/media/openai-news-corp-content-deal.html
  source_notes: notes here

- variable: total_articles__taylorandfrancis__count
  value: 140000.0
  units: articles
  value_description: Number of Taylor and Francis Articles
  variable_type: group_size
  confidence_in_number: moderate
  key_assumption: assuming each article has 1 unique author
  source_url: source needed
  source_notes: notes here

- variable: journalists__wsj__count
  value: 2000.0
  units: people
  value_description: Number of WSJ journalists
  variable_type: group_size
  confidence_in_number: moderate
  key_assumption: Not specified
  source_url: https://en.wikipedia.org/wiki/The_Wall_Street_Journal
  source_notes: Wikipedia article

- variable: total_employees__newscorp__count
  value: 25000.0
  units: people
  value_description: Number of News Corp employees
  variable_type: group_size
  confidence_in_number: moderate
  key_assumption: Not specified
  source_url: https://en.wikipedia.org/wiki/News_Corp
  source_notes: Wikipedia article (primary source is sec.gov)

- variable: total_population__world__people
  value: 8100000000.0
  units: people
  value_description: Number of people on Earth
  variable_type: group_size
  confidence_in_number: high
  key_assumption: Not specified
  source_url: https://www.worldometers.info/world-population/
  source_notes: Worldometer website
- variable: wikipedia_contribution_distribution
  value: .nan
  units: .nan
  value_description: .nan
  variable_type: .nan
  confidence_in_number: unknown
  key_assumption: Not specified
  source_url: https://p2pmodels.eu/power-law-distribution-characterize-wiki-communities/
  source_notes: No additional notes.

calculations:
  - title: "Distributing the 'value generated' by AI to everyone in the world"
    description: "If we were to immediately distribute AI revenue to everyone on Earth, how much would each person receive?"
    inputs:
      - yearly_revenue__openai__dollars
      - total_population__world__people
    result:
      label: "Dividend per person"
      units: dollars
      value: 0
    explanation: "yearly_revenue__openai__dollars / total_population__world__people"

  - title: "Commissiong a fresh LLM dataset"
    description: How much would it cost to pay for a brand new LLM-scale pre-training dataset assuming moderate freelance writing wages. 
    inputs:
      - words_per_token__generic__words_per_token
      - freelance_rate_per_word_high
      - dataset_size__llama3__tokens
    result:
      label: Total cost
      units: dollars
      value: 0
    explanation: "dataset_size__llama3__tokens * words_per_token__generic__words_per_token * freelance_rate_per_word_high"
  
  - title: "Distributing model 'value generated' per token"
    description: >
      Estimate a $/token value. Take some estimate of value generated (e.g., the revenue of an AI company) and divide by the
      number of tokens used to train that company's models.
    inputs:
      - yearly_revenue__openai__dollars
      - dataset_size__llama3__tokens
    result:
      label: "Revenue per Token"
      value: 0
    explanation: "yearly_revenue__openai__dollars / dataset_size__llama3__tokens"
  
  - title: "Average Contribution Size"
    description: "Estimate of the average contribution size in tokens based on the dataset size and number of contributions."
    inputs:
      - dataset_size__llama3__tokens
      - average_tokens_per_contribution__redpajama__tokens_per_contribution
    result:
      label: "Average Contribution Size"
      value: 0
    explanation: "dataset_size__llama3__tokens / average_tokens_per_contribution__redpajama__tokens_per_contribution"
  - title: "Revenue per contribution"
    description: "Calculate the revenue generated per contribution based on the average tokens per contribution and revenue per token."
    inputs:
      - average_tokens_per_contribution__redpajama__tokens_per_contribution
      - yearly_revenue__openai__dollars
      - dataset_size__llama3__tokens
    result:
      label: "Revenue per Contribution"
      value: 0
    explanation: "(yearly_revenue__openai__dollars / dataset_size__llama3__tokens) * average_tokens_per_contribution__redpajama__tokens_per_contribution"
  - title: "Revenue per Book in Books3"
    description: "Estimate the revenue generated per book in the Books3 dataset."
    inputs:
      - total_books__books3__books
      - average_length__book__words
      - yearly_revenue__openai__dollars
      - dataset_size__llama3__tokens
    result:
      label: "Revenue per Book"
      value: 0
    explanation: "(yearly_revenue__openai__dollars / dataset_size__llama3__tokens) * (total_books__books3__books * average_length__book__words)"
  - title: "Dataset Coverage Ratio"
    description: "Calculate the ratio of books in the Books3 dataset relative to the total number of tokens."
    inputs:
      - total_books__books3__books
      - average_length__book__words
      - dataset_size__llama3__tokens
    result:
      label: "Dataset Coverage Ratio"
      value: 0
    explanation: "(total_books__books3__books * average_length__book__words) / dataset_size__llama3__tokens"
