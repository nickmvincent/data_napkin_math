inputs:
- variable: total_tokens
  nice_name: Total pre-training tokens
  value: 15000000000000.0
  units: tokens
  scale: 1e9
  display_units: billions of tokens
  value_description: Total tokens used for pre-training
  variable_type: dataset
  confidence_in_number: moderate
  key_assumption: We can use 15 trillion as a default value based on the assumption that most frontier models use roughly same pre-training size.
  15T is the number cited in Llama3 model card and close to the FineWeb size.
  source_url: https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md
  source_notes: Source is the Llama 3 model card. It describes total number of tokens for pre-training.
  
- variable: ai_revenue
  nice_name: Revenue from AI
  value: 3490000000.0
  units: dollars
  scale: 1e6
  display_units: millions of dollars
  value_description: Value generated by AI in dollars
  variable_type: company
  confidence_in_number: moderate
  key_assumption: assuming all 2024 OpenAI revenue is from LLMs
  source_url: https://www.bloomberg.com/news/articles/2024-06-12/openai-doubles-annualized-revenue-to-3-4-billion-information
  source_notes: business news coverage of openai

- variable: ai_revenue2
  nice_name: Revenue from AI (Anthropic)
  value: 1000000000.0
  units: dollars
  scale: 1e6
  display_units: millions of dollars
  value_description: Value generated by AI in dollars
  variable_type: company
  confidence_in_number: moderate
  key_assumption: assuming all 2024 Anthropic revenue is from LLMs
  source_url: https://www.pymnts.com/artificial-intelligence-2/2024/anthropic-revenue-reportedly-set-to-jump-to-1-billion-this-year/
  source_notes: business news coverage of anthropic

- variable: average_tokens_per_contribution
  nice_name: Average number of a token in a single document
  value: 1413.0
  units: tokens / contribution
  value_description: Average number of tokens in a single 'contribution'
  variable_type: dataset
  confidence_in_number: high
  key_assumption: working with averages here
  source_url: https://github.com/togethercomputer/RedPajama-Data
  source_notes: RedPajama readme reports a ratio of documents to tokens (after dedupe). We use the English figures (20.5T tokens / 14.5B documents)

- variable: num_books_in_books3
  value: 196640.0
  units: books
  value_description: Number of books in Books3
  variable_type: dataset
  confidence_in_number: high
  key_assumption: N/A
  source_url: https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/books3-ai-training-dataset
  source_notes: Entry in the AIAAIC repository.

- variable: average_book_length_words
  value: 80000.0
  units: words
  value_description: An average number of words per book
  variable_type: dataset
  confidence_in_number: moderate
  key_assumption: Average number of words per book
  source_url: https://www.penguin.co.uk/articles/2020/09/book-length-debate-fiction-long-novels
  source_notes: No additional notes.

- variable: words_per_token
  value: 0.75
  units: words / token
  value_description: average number of words per token
  variable_type: training_detail
  confidence_in_number: moderate
  key_assumption: Average across random queries
  source_url: https://platform.openai.com/tokenizer
  source_notes: OpenAI Tokenizer

- variable: freelance_rate_per_word_low
  value: 0.05
  units: dollars / word
  value_description: Freelance rate per word (low estimate)
  variable_type: wage_data
  confidence_in_number: unknown
  key_assumption: '...'
  source_url: source needed
  source_notes: source needed

- variable: freelance_rate_per_word_high
  value: 0.1
  units: dollars / word
  value_description: Freelance rate per word (high estimate)
  variable_type: wage_date
  confidence_in_number: unknown
  key_assumption: assumptions needed
  source_url: source needed
  source_notes: source needed
- variable: freelance_rate_per_word_very_high
  value: 1.0
  units: dollars / word
  value_description: Freelance rate per word (very high estimate)
  variable_type: wage_data
  confidence_in_number: unknown
  key_assumption: assumptions needed
  source_url: source needed
  source_notes: source needed
- variable: reddit_users
  value: 267500.0
  units: thousands of daily active users
  value_description: Number of Reddit daily active users
  variable_type: group_size
  confidence_in_number: high
  key_assumption: None
  source_url: https://backlinko.com/reddit-users
  source_notes: 'article about # of reddit users'
- variable: reddit_google_deal_value
  value: 60.0
  units: milliosn fo dollars
  value_description: Payment made to Reddit by Google
  variable_type: deal_data
  confidence_in_number: high
  key_assumption: None
  source_url: https://www.cbsnews.com/news/google-reddit-60-million-deal-ai-training/
  source_notes: CBS news coverage
- variable: taylorandfrancis_microsoft_deal_value
  value: 10.0
  units: millions of dollars
  value_description: Value of the Taylor and Francis deal
  variable_type: deal_data
  confidence_in_number: high
  key_assumption: None
  source_url: https://www.thebookseller.com/news/academic-authors-shocked-after-taylor--francis-sells-access-to-their-research-to-microsoft-ai
  source_notes: news coverage of deal
- variable: newscorp_deal_value
  value: 50000000.0
  units: dollars
  value_description: Value of the News Corp deal
  variable_type: deal_date
  confidence_in_number: high
  key_assumption: Not specified
  source_url: https://www.nytimes.com/2024/05/22/business/media/openai-news-corp-content-deal.html
  source_notes: notes here
- variable: tf_users
  value: 140000.0
  units: .nan
  value_description: Number of Taylor and Francis Articles
  variable_type: group_size
  confidence_in_number: moderate
  key_assumption: assuming each article has 1 unique author
  source_url: source needed
  source_notes: notes here
- variable: wsj_journalists
  value: 2000.0
  units: .nan
  value_description: Number of WSJ journalists
  variable_type: group_size
  confidence_in_number: moderate
  key_assumption: Not specified
  source_url: https://en.wikipedia.org/wiki/The_Wall_Street_Journal
  source_notes: Wikipedia article
- variable: newscorp_num_employees
  value: 25000.0
  units: .nan
  value_description: Number of News Corp employees
  variable_type: group_size
  confidence_in_number: moderate
  key_assumption: Not specified
  source_url: https://en.wikipedia.org/wiki/News_Corp
  source_notes: Wikipedia article (primary source is sec.gov)
- variable: global_num_people
  value: 8100000000.0
  units: .nan
  value_description: Number of people on Earth
  variable_type: group_size
  confidence_in_number: high
  key_assumption: Not specified
  source_url: https://www.worldometers.info/world-population/
  source_notes: Worldometer website
- variable: wikipedia_contribution_distribution
  value: .nan
  units: .nan
  value_description: .nan
  variable_type: .nan
  confidence_in_number: unknown
  key_assumption: Not specified
  source_url: https://p2pmodels.eu/power-law-distribution-characterize-wiki-communities/
  source_notes: No additional notes.

calculations:
  - title: "Distributing the 'value generated' by AI to everyone in the world"
    description: "If we were to immediately distribute AI revenue to everyone on Earth, how much would each person receive?"
    inputs:
      - ai_revenue
      - global_num_people
    result:
      label: "Dividend per person"
      units: dollars
      value: 0
    explanation: "ai_revenue / global_num_people"

  - title: "Commissiong a fresh LLM dataset"
    description: How much would it cost to pay for a brand new LLM-scale pre-training dataset assuming moderate freelance writing wages. 
    inputs:
      - words_per_token
      - freelance_rate_per_word_high
      - total_tokens
    result:
      label: Total cost
      units: dollars
      value: 0
    explanation: "total_tokens * words_per_token * freelance_rate_per_word_high"
  
  - title: "Distributing model 'value generated' per token"
    description: "Estimate a $/token value. Take some estimate of value generated (e.g., the revenue of an AI company) and divide by the
    number of tokens used to train that company's models."
    inputs:
      - ai_revenue
      - total_tokens
    result:
      label: "Revenue per Token"
      value: 0
    explanation: "ai_revenue / total_tokens"
  
  - title: "Average Contribution Size"
    description: "Estimate of the average contribution size in tokens based on the dataset size and number of contributions."
    inputs:
      - total_tokens
      - average_tokens_per_contribution
    result:
      label: "Average Contribution Size"
      value: 0
    explanation: "total_tokens / average_tokens_per_contribution"
  - title: "Revenue per contribution"
    description: "Calculate the revenue generated per contribution based on the average tokens per contribution and revenue per token."
    inputs:
      - average_tokens_per_contribution
      - ai_revenue
      - total_tokens
    result:
      label: "Revenue per Contribution"
      value: 0
    explanation: "(ai_revenue / total_tokens) * average_tokens_per_contribution"
  - title: "Revenue per Book in Books3"
    description: "Estimate the revenue generated per book in the Books3 dataset."
    inputs:
      - num_books_in_books3
      - average_book_length_words
      - ai_revenue
      - total_tokens
    result:
      label: "Revenue per Book"
      value: 0
    explanation: "(ai_revenue / total_tokens) * (num_books_in_books3 * average_book_length_words)"
  - title: "Dataset Coverage Ratio"
    description: "Calculate the ratio of books in the Books3 dataset relative to the total number of tokens."
    inputs:
      - num_books_in_books3
      - average_book_length_words
      - total_tokens
    result:
      label: "Dataset Coverage Ratio"
      value: 0
    explanation: "(num_books_in_books3 * average_book_length_words) / total_tokens"
