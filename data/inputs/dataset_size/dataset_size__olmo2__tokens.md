---
title: Total pre-training tokens (Olmo 2)
value: 5000000000000
scale: 1000000000
display_units: billions of tokens
variable_name: dataset_size__olmo2__tokens
variable_type: dataset_size
entity: olmo2
units: tokens
source_url: https://huggingface.co/allenai/OLMo-2-1124-7B
date_added: 2025-03-19
tags:
  - dataset_size
  - olmo2
  - tokens
---

# Total pre-training tokens (Olmo 2)

**Value:** 5,000 billions of tokens

## Description

Total tokens used to pre-training a model

## Key Assumption

...

## Source

- [https://huggingface.co/allenai/OLMo-2-1124-7B](https://huggingface.co/allenai/OLMo-2-1124-7B)
- Source is the Olmo 2 model card. It describes total number of tokens for pre-training.
