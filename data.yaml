inputs:
- variable: total_tokens
  value: 15000000000000.0
  units: tokens
  value_description: total tokens in pre-training
  variable_type: dataset
  confidence_in_number: moderate
  key_assumption: assuming all frontier models use roughly same pre-training size,
    we can use Llama3 and FineWeb numbers as our representative figure
  source_url: https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md
  source_notes: Llama 3 model card describes total number of tokens for pre-training
- variable: model_value
  value: 3490000000.0
  units: dollars
  value_description: Value of the model in dollars
  variable_type: revenue_data
  confidence_in_number: moderate
  key_assumption: assuming all 2024 OpenAI revenue is from LLMs
  source_url: https://www.bloomberg.com/news/articles/2024-06-12/openai-doubles-annualized-revenue-to-3-4-billion-information
  source_notes: business news coverage of openai
- variable: average_tokens_per_contribution
  value: 1500.0
  units: tokens / contribution
  value_description: Average number of tokens per contribution
  variable_type: dataset
  confidence_in_number: unknown
  key_assumption: working with averages here
  source_url: https://github.com/togethercomputer/RedPajama-Data
  source_notes: RedPajama readme reports a ratio of documents to tokens (after dedupe)
- variable: num_books_in_books3
  value: 200000.0
  units: books
  value_description: https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/books3-ai-training-dataset
  variable_type: dataset
  confidence_in_number: unknown
  key_assumption: Number of books in Books3
  source_url: https://placeholder-link.com
  source_notes: No additional notes.
- variable: average_book_length_words
  value: 90000.0
  units: words
  value_description: https://gitnux.org/average-book-length/
  variable_type: dataset
  confidence_in_number: unknown
  key_assumption: Average number of tokens per book
  source_url: https://placeholder-link.com
  source_notes: No additional notes.
- variable: words_per_token
  value: 0.75
  units: words / token
  value_description: average number of words per token
  variable_type: training_detail
  confidence_in_number: unknown
  key_assumption: '...'
  source_url: https://platform.openai.com/tokenizer
  source_notes: OpenAI Tokenizer
- variable: freelance_rate_per_word_low
  value: 0.05
  units: dollars / word
  value_description: Freelance rate per word (low estimate)
  variable_type: wage_data
  confidence_in_number: unknown
  key_assumption: '...'
  source_url: source needed
  source_notes: source needed
- variable: freelance_rate_per_word_high
  value: 0.1
  units: dollars / word
  value_description: Freelance rate per word (high estimate)
  variable_type: wage_date
  confidence_in_number: unknown
  key_assumption: assumptions needed
  source_url: source needed
  source_notes: source needed
- variable: freelance_rate_per_word_very_high
  value: 1.0
  units: dollars / word
  value_description: Freelance rate per word (very high estimate)
  variable_type: wage_data
  confidence_in_number: unknown
  key_assumption: assumptions needed
  source_url: source needed
  source_notes: source needed
- variable: reddit_users
  value: 267500000.0
  units: users
  value_description: Number of Reddit users
  variable_type: group_size
  confidence_in_number: high
  key_assumption: split between all users
  source_url: https://backlinko.com/reddit-users
  source_notes: 'article about # reddit users'
- variable: reddit_deal_value
  value: 60000000.0
  units: dollars
  value_description: Payment made to reddit by Google
  variable_type: deal_data
  confidence_in_number: high
  key_assumption: Not specified
  source_url: source needed
  source_notes: notes here
- variable: tf_deal_value
  value: 10000000.0
  units: dollars
  value_description: Value of the Taylor and Francis deal
  variable_type: deal_data
  confidence_in_number: high
  key_assumption: Not specified
  source_url: https://www.thebookseller.com/news/academic-authors-shocked-after-taylor--francis-sells-access-to-their-research-to-microsoft-ai
  source_notes: news coverage
- variable: newscorp_deal_value
  value: 50000000.0
  units: dollars
  value_description: Value of the News Corp deal
  variable_type: deal_date
  confidence_in_number: high
  key_assumption: Not specified
  source_url: https://www.nytimes.com/2024/05/22/business/media/openai-news-corp-content-deal.html
  source_notes: notes here
- variable: tf_users
  value: 140000.0
  units: .nan
  value_description: Number of Taylor and Francis Articles
  variable_type: group_size
  confidence_in_number: moderate
  key_assumption: assuming each article has 1 unique author
  source_url: source needed
  source_notes: notes here
- variable: wsj_journalists
  value: 2000.0
  units: .nan
  value_description: Number of WSJ journalists
  variable_type: group_size
  confidence_in_number: moderate
  key_assumption: Not specified
  source_url: https://en.wikipedia.org/wiki/The_Wall_Street_Journal
  source_notes: Wikipedia article
- variable: newscorp_num_employees
  value: 25000.0
  units: .nan
  value_description: Number of News Corp employees
  variable_type: group_size
  confidence_in_number: moderate
  key_assumption: Not specified
  source_url: https://en.wikipedia.org/wiki/News_Corp
  source_notes: Wikipedia article (primary source is sec.gov)
- variable: global_num_people
  value: 8100000000.0
  units: .nan
  value_description: Number of people on Earth
  variable_type: group_size
  confidence_in_number: high
  key_assumption: Not specified
  source_url: https://www.worldometers.info/world-population/
  source_notes: Worldometer website
- variable: wikipedia_contribution_distribution
  value: .nan
  units: .nan
  value_description: .nan
  variable_type: .nan
  confidence_in_number: unknown
  key_assumption: Not specified
  source_url: https://p2pmodels.eu/power-law-distribution-characterize-wiki-communities/
  source_notes: No additional notes.
calculations:
  - title: "Distributing model 'value' based on tokens"
    description: "Estimate the revenue generated per token based on model value and total tokens. If we were to immediately distribute AI revenue to data creators, you could multiply this by your number of tokens contributed to see how much you'd earn."
    inputs:
      - model_value
      - total_tokens
    result:
      label: "Revenue per Token"
      value: 0
    explanation: "model_value / total_tokens"
    diagram: "Divide20by4.svg.png"
  - title: "Average Contribution Size"
    description: "Estimate the average contribution size in tokens based on the dataset size and number of contributions."
    inputs:
      - total_tokens
      - average_tokens_per_contribution
    result:
      label: "Average Contribution Size"
      value: 0
    explanation: "total_tokens / average_tokens_per_contribution"
  - title: "Dataset Contribution Revenue"
    description: "Calculate the revenue generated per contribution based on the average tokens per contribution and revenue per token."
    inputs:
      - average_tokens_per_contribution
      - model_value
      - total_tokens
    result:
      label: "Revenue per Contribution"
      value: 0
    explanation: "(model_value / total_tokens) * average_tokens_per_contribution"
  - title: "Revenue per Book in Books3"
    description: "Estimate the revenue generated per book in the Books3 dataset."
    inputs:
      - num_books_in_books3
      - average_book_length_words
      - model_value
      - total_tokens
    result:
      label: "Revenue per Book"
      value: 0
    explanation: "(model_value / total_tokens) * (num_books_in_books3 * average_book_length_words)"
  - title: "Dataset Coverage Ratio"
    description: "Calculate the ratio of books in the Books3 dataset relative to the total number of tokens."
    inputs:
      - num_books_in_books3
      - average_book_length_words
      - total_tokens
    result:
      label: "Dataset Coverage Ratio"
      value: 0
    explanation: "(num_books_in_books3 * average_book_length_words) / total_tokens"
